{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "505f61e2",
   "metadata": {},
   "source": [
    "# Multi-class Classification using Logistic Regression\n",
    "\n",
    "Discriminative multi-class classification aims to learn decision boundaries that effectively separate multiple \n",
    "classes within a dataset. Three common methods for discriminative multi-class classification are one-vs-one, \n",
    "one-vs-all and softmax regression.\n",
    "\n",
    "- One-vs-One (OvO) trains several binary logistic classifiers, each specialized in distinguishing between \n",
    "a specific pair of classes, and predicts the class based on their combined outcomes. \n",
    "\n",
    "- One-vs-All (OvA) builds one binary classifier per class against all remaining classes and assigns the \n",
    "label with the highest confidence score.\n",
    "\n",
    "- Softmax regression extends logistic regression to the multi-class setting by modeling a normalized \n",
    "probability distribution over all classes and selecting the class with the maximum probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3666d087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "680cbaa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280_od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0      14.23        1.71  2.43               15.6        127           2.80   \n",
       "1      13.20        1.78  2.14               11.2        100           2.65   \n",
       "2      13.16        2.36  2.67               18.6        101           2.80   \n",
       "3      14.37        1.95  2.50               16.8        113           3.85   \n",
       "4      13.24        2.59  2.87               21.0        118           2.80   \n",
       "..       ...         ...   ...                ...        ...            ...   \n",
       "173    13.71        5.65  2.45               20.5         95           1.68   \n",
       "174    13.40        3.91  2.48               23.0        102           1.80   \n",
       "175    13.27        4.28  2.26               20.0        120           1.59   \n",
       "176    13.17        2.59  2.37               20.0        120           1.65   \n",
       "177    14.13        4.10  2.74               24.5         96           2.05   \n",
       "\n",
       "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0          3.06                  0.28             2.29             5.64  1.04   \n",
       "1          2.76                  0.26             1.28             4.38  1.05   \n",
       "2          3.24                  0.30             2.81             5.68  1.03   \n",
       "3          3.49                  0.24             2.18             7.80  0.86   \n",
       "4          2.69                  0.39             1.82             4.32  1.04   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "173        0.61                  0.52             1.06             7.70  0.64   \n",
       "174        0.75                  0.43             1.41             7.30  0.70   \n",
       "175        0.69                  0.43             1.35            10.20  0.59   \n",
       "176        0.68                  0.53             1.46             9.30  0.60   \n",
       "177        0.76                  0.56             1.35             9.20  0.61   \n",
       "\n",
       "     od280_od315_of_diluted_wines  proline  class_label  \n",
       "0                            3.92     1065            1  \n",
       "1                            3.40     1050            1  \n",
       "2                            3.17     1185            1  \n",
       "3                            3.45     1480            1  \n",
       "4                            2.93      735            1  \n",
       "..                            ...      ...          ...  \n",
       "173                          1.74      740            3  \n",
       "174                          1.56      750            3  \n",
       "175                          1.56      835            3  \n",
       "176                          1.62      840            3  \n",
       "177                          1.60      560            3  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data/wine_dataset.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4720ce52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 178 entries, 0 to 177\n",
      "Data columns (total 14 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   alcohol                       178 non-null    float64\n",
      " 1   malic_acid                    178 non-null    float64\n",
      " 2   ash                           178 non-null    float64\n",
      " 3   alcalinity_of_ash             178 non-null    float64\n",
      " 4   magnesium                     178 non-null    int64  \n",
      " 5   total_phenols                 178 non-null    float64\n",
      " 6   flavanoids                    178 non-null    float64\n",
      " 7   nonflavanoid_phenols          178 non-null    float64\n",
      " 8   proanthocyanins               178 non-null    float64\n",
      " 9   color_intensity               178 non-null    float64\n",
      " 10  hue                           178 non-null    float64\n",
      " 11  od280_od315_of_diluted_wines  178 non-null    float64\n",
      " 12  proline                       178 non-null    int64  \n",
      " 13  class_label                   178 non-null    int64  \n",
      "dtypes: float64(11), int64(3)\n",
      "memory usage: 19.6 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d1391286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 14)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f578f3",
   "metadata": {},
   "source": [
    "well as always we have the std and mean in the describe but let's do them ourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3c0469dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280_od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>13.000618</td>\n",
       "      <td>2.336348</td>\n",
       "      <td>2.366517</td>\n",
       "      <td>19.494944</td>\n",
       "      <td>99.741573</td>\n",
       "      <td>2.295112</td>\n",
       "      <td>2.029270</td>\n",
       "      <td>0.361854</td>\n",
       "      <td>1.590899</td>\n",
       "      <td>5.058090</td>\n",
       "      <td>0.957449</td>\n",
       "      <td>2.611685</td>\n",
       "      <td>746.893258</td>\n",
       "      <td>1.938202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.811827</td>\n",
       "      <td>1.117146</td>\n",
       "      <td>0.274344</td>\n",
       "      <td>3.339564</td>\n",
       "      <td>14.282484</td>\n",
       "      <td>0.625851</td>\n",
       "      <td>0.998859</td>\n",
       "      <td>0.124453</td>\n",
       "      <td>0.572359</td>\n",
       "      <td>2.318286</td>\n",
       "      <td>0.228572</td>\n",
       "      <td>0.709990</td>\n",
       "      <td>314.907474</td>\n",
       "      <td>0.775035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>11.030000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>1.280000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>1.270000</td>\n",
       "      <td>278.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12.362500</td>\n",
       "      <td>1.602500</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>1.742500</td>\n",
       "      <td>1.205000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>3.220000</td>\n",
       "      <td>0.782500</td>\n",
       "      <td>1.937500</td>\n",
       "      <td>500.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.050000</td>\n",
       "      <td>1.865000</td>\n",
       "      <td>2.360000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>2.355000</td>\n",
       "      <td>2.135000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>1.555000</td>\n",
       "      <td>4.690000</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>673.500000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.677500</td>\n",
       "      <td>3.082500</td>\n",
       "      <td>2.557500</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>3.170000</td>\n",
       "      <td>985.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.830000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.230000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>3.880000</td>\n",
       "      <td>5.080000</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>3.580000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.710000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1680.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          alcohol  malic_acid         ash  alcalinity_of_ash   magnesium  \\\n",
       "count  178.000000  178.000000  178.000000         178.000000  178.000000   \n",
       "mean    13.000618    2.336348    2.366517          19.494944   99.741573   \n",
       "std      0.811827    1.117146    0.274344           3.339564   14.282484   \n",
       "min     11.030000    0.740000    1.360000          10.600000   70.000000   \n",
       "25%     12.362500    1.602500    2.210000          17.200000   88.000000   \n",
       "50%     13.050000    1.865000    2.360000          19.500000   98.000000   \n",
       "75%     13.677500    3.082500    2.557500          21.500000  107.000000   \n",
       "max     14.830000    5.800000    3.230000          30.000000  162.000000   \n",
       "\n",
       "       total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
       "count     178.000000  178.000000            178.000000       178.000000   \n",
       "mean        2.295112    2.029270              0.361854         1.590899   \n",
       "std         0.625851    0.998859              0.124453         0.572359   \n",
       "min         0.980000    0.340000              0.130000         0.410000   \n",
       "25%         1.742500    1.205000              0.270000         1.250000   \n",
       "50%         2.355000    2.135000              0.340000         1.555000   \n",
       "75%         2.800000    2.875000              0.437500         1.950000   \n",
       "max         3.880000    5.080000              0.660000         3.580000   \n",
       "\n",
       "       color_intensity         hue  od280_od315_of_diluted_wines      proline  \\\n",
       "count       178.000000  178.000000                    178.000000   178.000000   \n",
       "mean          5.058090    0.957449                      2.611685   746.893258   \n",
       "std           2.318286    0.228572                      0.709990   314.907474   \n",
       "min           1.280000    0.480000                      1.270000   278.000000   \n",
       "25%           3.220000    0.782500                      1.937500   500.500000   \n",
       "50%           4.690000    0.965000                      2.780000   673.500000   \n",
       "75%           6.200000    1.120000                      3.170000   985.000000   \n",
       "max          13.000000    1.710000                      4.000000  1680.000000   \n",
       "\n",
       "       class_label  \n",
       "count   178.000000  \n",
       "mean      1.938202  \n",
       "std       0.775035  \n",
       "min       1.000000  \n",
       "25%       1.000000  \n",
       "50%       2.000000  \n",
       "75%       3.000000  \n",
       "max       3.000000  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "90b24b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alcohol                         float64\n",
       "malic_acid                      float64\n",
       "ash                             float64\n",
       "alcalinity_of_ash               float64\n",
       "magnesium                         int64\n",
       "total_phenols                   float64\n",
       "flavanoids                      float64\n",
       "nonflavanoid_phenols            float64\n",
       "proanthocyanins                 float64\n",
       "color_intensity                 float64\n",
       "hue                             float64\n",
       "od280_od315_of_diluted_wines    float64\n",
       "proline                           int64\n",
       "class_label                       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3fedb44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alcohol                           0.811827\n",
       "malic_acid                        1.117146\n",
       "ash                               0.274344\n",
       "alcalinity_of_ash                 3.339564\n",
       "magnesium                        14.282484\n",
       "total_phenols                     0.625851\n",
       "flavanoids                        0.998859\n",
       "nonflavanoid_phenols              0.124453\n",
       "proanthocyanins                   0.572359\n",
       "color_intensity                   2.318286\n",
       "hue                               0.228572\n",
       "od280_od315_of_diluted_wines      0.709990\n",
       "proline                         314.907474\n",
       "class_label                       0.775035\n",
       "dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "12126227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alcohol                          13.000618\n",
       "malic_acid                        2.336348\n",
       "ash                               2.366517\n",
       "alcalinity_of_ash                19.494944\n",
       "magnesium                        99.741573\n",
       "total_phenols                     2.295112\n",
       "flavanoids                        2.029270\n",
       "nonflavanoid_phenols              0.361854\n",
       "proanthocyanins                   1.590899\n",
       "color_intensity                   5.058090\n",
       "hue                               0.957449\n",
       "od280_od315_of_diluted_wines      2.611685\n",
       "proline                         746.893258\n",
       "class_label                       1.938202\n",
       "dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dc65f250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280_od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6        127           2.80   \n",
       "1    13.20        1.78  2.14               11.2        100           2.65   \n",
       "2    13.16        2.36  2.67               18.6        101           2.80   \n",
       "3    14.37        1.95  2.50               16.8        113           3.85   \n",
       "4    13.24        2.59  2.87               21.0        118           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280_od315_of_diluted_wines  proline  class_label  \n",
       "0                          3.92     1065            1  \n",
       "1                          3.40     1050            1  \n",
       "2                          3.17     1185            1  \n",
       "3                          3.45     1480            1  \n",
       "4                          2.93      735            1  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dd825c",
   "metadata": {},
   "source": [
    "NO Categorical Features But We could check like this too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cf2f0ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. No categorical features (all numerical)\n"
     ]
    }
   ],
   "source": [
    "categorical_features = data.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "if categorical_features:\n",
    "    for feature in categorical_features:\n",
    "        if feature != 'target' and 'class' not in feature.lower():\n",
    "            print(f\"   {feature}: {data[feature].nunique()} unique values\")\n",
    "else:\n",
    "    print(f\"\\n4. No categorical features (all numerical)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f8967c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Shape: (178, 14)\n",
      "  Missing values: 0\n",
      "  Duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "df_processed = data.copy()\n",
    "\n",
    "print(f\"  Shape: {df_processed.shape}\")\n",
    "print(f\"  Missing values: {df_processed.isnull().sum().sum()}\")\n",
    "print(f\"  Duplicates: {df_processed.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ffddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. Target variable: class_label\n",
      "Classes: [np.int64(1), np.int64(2), np.int64(3)]\n",
      "Class distribution:\n",
      "Class 1: 59 samples (33.1%)\n",
      "Class 2: 71 samples (39.9%)\n",
      "Class 3: 48 samples (27.0%)\n",
      "\n",
      "✓ Dataset is clean! No preprocessing needed beyond normalization.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "target_col = 'class_label'\n",
    "\n",
    "print(f\"\\n7. Target variable: {target_col}\")\n",
    "print(f\"Classes: {sorted(data[target_col].unique())}\")\n",
    "print(f\"Class distribution:\")\n",
    "class_counts = data[target_col].value_counts().sort_index()\n",
    "for cls, count in class_counts.items():\n",
    "    print(f\"Class {cls}: {count} samples ({100*count/len(data):.1f}%)\")\n",
    "\n",
    "print(f\"\\nDataset is clean! No preprocessing needed beyond normalization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f814ca85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (178, 13) (samples or m, features or n)\n",
      "y shape: (178,) (samples or m, )\n"
     ]
    }
   ],
   "source": [
    "X = data.drop(target_col, axis=1).values\n",
    "y = data[target_col].values\n",
    "\n",
    "print(f\"X shape: {X.shape} (samples or m, features or n)\")\n",
    "print(f\"y shape: {y.shape} (samples or m, )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3db57434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of each feature:\n",
      "  alcohol: mean=13.0006, std=0.8095\n",
      "  malic_acid: mean=2.3363, std=1.1140\n",
      "  ash: mean=2.3665, std=0.2736\n",
      "  alcalinity_of_ash: mean=19.4949, std=3.3302\n",
      "  magnesium: mean=99.7416, std=14.2423\n",
      "  total_phenols: mean=2.2951, std=0.6241\n",
      "  flavanoids: mean=2.0293, std=0.9960\n",
      "  nonflavanoid_phenols: mean=0.3619, std=0.1241\n",
      "  proanthocyanins: mean=1.5909, std=0.5707\n",
      "  color_intensity: mean=5.0581, std=2.3118\n",
      "  hue: mean=0.9574, std=0.2279\n",
      "  od280_od315_of_diluted_wines: mean=2.6117, std=0.7080\n",
      "  proline: mean=746.8933, std=314.0217\n"
     ]
    }
   ],
   "source": [
    "# We could do it this way too!\n",
    "print(f\"\\nMean of each feature:\")\n",
    "for i, col in enumerate(data.columns[:-1]):\n",
    "    print(f\"  {col}: mean={X[:, i].mean():.4f}, std={X[:, i].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011b911e",
   "metadata": {},
   "source": [
    "### Data Split with Stratified Train-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1b5817f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "unique_classes = np.unique(y)\n",
    "\n",
    "train_idx = []\n",
    "test_idx = []\n",
    "\n",
    "# Splitting Each Class separately, so we maintain the proportions:\n",
    "for cls in unique_classes:\n",
    "    cls_indices = np.where(y == cls)[0]\n",
    "    split_point = int(0.75 * len(cls_indices))\n",
    "    \n",
    "    np.random.shuffle(cls_indices)\n",
    "    train_idx.extend(cls_indices[:split_point])\n",
    "    test_idx.extend(cls_indices[split_point:])\n",
    "\n",
    "train_idx = np.array(train_idx)\n",
    "test_idx = np.array(train_idx)\n",
    "\n",
    "X_train = X[train_idx]\n",
    "y_train = y[train_idx]\n",
    "\n",
    "X_test = X[test_idx]\n",
    "y_test = y[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d641ef30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution preserved:\n",
      "  Training set:\n",
      "Class 1: 44 (33.1%)\n",
      "Class 2: 53 (39.8%)\n",
      "Class 3: 36 (27.1%)\n",
      "  Test set:\n",
      "Class 1: 44 (33.1%)\n",
      "Class 2: 53 (39.8%)\n",
      "Class 3: 36 (27.1%)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nClass distribution preserved:\")\n",
    "print(f\"  Training set:\")\n",
    "for cls in unique_classes:\n",
    "    count = np.sum(y_train == cls)\n",
    "    print(f\"Class {cls}: {count} ({100*count/len(y_train):.1f}%)\")\n",
    "\n",
    "print(f\"  Test set:\")\n",
    "for cls in unique_classes:\n",
    "    count = np.sum(y_test == cls)\n",
    "    print(f\"Class {cls}: {count} ({100*count/len(y_test):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9733a508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before outlier removal: X_train shape = (133, 13)\n",
      "Outliers detected: 11\n",
      "After outlier removal: X_train shape = (122, 13)\n",
      "Samples removed: 11\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nBefore outlier removal: X_train shape = {X_train.shape}\")\n",
    "\n",
    "# Z-scores using training set statistics\n",
    "train_mean = np.mean(X_train, axis=0)\n",
    "train_std = np.std(X_train, axis=0)\n",
    "\n",
    "Z_scores = np.abs((X_train - train_mean) / (train_std + 1e-8))\n",
    "\n",
    "# outliers (any feature with |z| > 2.75)\n",
    "outlier_mask = np.any(Z_scores > 2.75, axis=1)\n",
    "n_outliers = np.sum(outlier_mask)\n",
    "\n",
    "print(f\"Outliers detected: {n_outliers}\")\n",
    "\n",
    "# Removing outliers\n",
    "X_train_clean = X_train[~outlier_mask]\n",
    "y_train_clean = y_train[~outlier_mask]\n",
    "\n",
    "print(f\"After outlier removal: X_train shape = {X_train_clean.shape}\")\n",
    "print(f\"Samples removed: {n_outliers}\")\n",
    "\n",
    "X_train = X_train_clean\n",
    "y_train = y_train_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "73e621f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before normalization:\n",
      "  X_train range: [0.13, 1515.00]\n",
      "After normalization:\n",
      "  X_train_normalized range: [0.00, 1.00]\n",
      "  All values now in [0, 1] range ✓\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nBefore normalization:\")\n",
    "print(f\"  X_train range: [{X_train.min():.2f}, {X_train.max():.2f}]\")\n",
    "\n",
    "# min/max\n",
    "X_train_min = np.min(X_train, axis=0)\n",
    "X_train_max = np.max(X_train, axis=0)\n",
    "\n",
    "# Avoid division by zero\n",
    "X_range = X_train_max - X_train_min\n",
    "X_range = np.where(X_range == 0, 1, X_range)\n",
    "\n",
    "# Apply min-max normalization: (X - min) / (max - min)\n",
    "X_train_normalized = (X_train - X_train_min) / X_range\n",
    "X_test_normalized = (X_test - X_train_min) / X_range\n",
    "\n",
    "print(f\"After normalization:\")\n",
    "print(f\"  X_train_normalized range: [{X_train_normalized.min():.2f}, {X_train_normalized.max():.2f}]\")\n",
    "print(f\"  All values now in [0, 1] range ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "09424827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression: \n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.theta = None\n",
    "        self.cost_history = []\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        z = np.clip(z, -500, 500) # limit the values in a NumPy array to a specified minimum and maximum range. Values outside this interval are replaced by the interval's edge values. \n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        \n",
    "        X_with_bias = np.column_stack([np.ones(m), X])\n",
    "        self.theta = np.zeros(X_with_bias.shape[1])\n",
    "        \n",
    "        for iteration in range(self.n_iterations):\n",
    "            z = X_with_bias @ self.theta\n",
    "            h_theta = self.sigmoid(z)\n",
    "            \n",
    "            errors = h_theta - y\n",
    "            # Update with gradient\n",
    "            gradients = (1/m) * (X_with_bias.T @ errors)\n",
    "            self.theta -= self.learning_rate * gradients\n",
    "            \n",
    "            if iteration % 100 == 0:\n",
    "                h_theta_clipped = np.clip(h_theta, 1e-15, 1 - 1e-15)\n",
    "                cost = -1/m * np.sum(y * np.log(h_theta_clipped) + (1 - y) * np.log(1 - h_theta_clipped))\n",
    "                self.cost_history.append(cost)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        m = len(X)\n",
    "        X_with_bias = np.column_stack([np.ones(m), X])\n",
    "        z = X_with_bias @ self.theta\n",
    "        return self.sigmoid(z)\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.predict_prob(X) >= threshold).astype(int)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a1fa3337",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxLogisticRegression:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.theta = None\n",
    "        self.cost_history = []\n",
    "        self.classes = None\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        \"\"\"Softmax activation function\"\"\"\n",
    "        z = z - np.max(z, axis=1, keepdims=True)\n",
    "        exp_z = np.exp(z)\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit multi-class logistic regression using softmax\"\"\"\n",
    "        m, _ = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        \n",
    "        # Add bias term\n",
    "        X_with_bias = np.column_stack([np.ones(m), X])\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.theta = np.random.randn(X_with_bias.shape[1], n_classes) * 0.01\n",
    "        \n",
    "        # Gradient descent\n",
    "        for iteration in range(self.n_iterations):\n",
    "            z = X_with_bias @ self.theta\n",
    "            h_theta = self.softmax(z)\n",
    "            \n",
    "            # Convert y to one-hot encoding\n",
    "            y_one_hot = np.zeros((m, n_classes))\n",
    "            for i, cls in enumerate(self.classes):\n",
    "                y_one_hot[y == cls, i] = 1\n",
    "            \n",
    "            # Cross-entropy loss\n",
    "            h_theta_clipped = np.clip(h_theta, 1e-15, 1 - 1e-15)\n",
    "            cost = -1/m * np.sum(y_one_hot * np.log(h_theta_clipped))\n",
    "            \n",
    "            # Gradients\n",
    "            errors = h_theta - y_one_hot\n",
    "            gradients = (1/m) * (X_with_bias.T @ errors)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.theta -= self.learning_rate * gradients\n",
    "            \n",
    "            if iteration % 100 == 0:\n",
    "                self.cost_history.append(cost)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        \"\"\"Predict class probabilities\"\"\"\n",
    "        m = len(X)\n",
    "        X_with_bias = np.column_stack([np.ones(m), X])\n",
    "        z = X_with_bias @ self.theta\n",
    "        return self.softmax(z)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        prob = self.predict_prob(X)\n",
    "        class_indices = np.argmax(prob, axis=1)\n",
    "        return self.classes[class_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9968b30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneVsAll:\n",
    "    def __init__ (self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.model = {}\n",
    "        self.classes = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        \n",
    "        print(f\"\\nTraining {len(self.classes)} OvA binary classifiers...\")\n",
    "        for cls in self.classes:\n",
    "            y_binary = (y == cls).astype(int)\n",
    "            \n",
    "            model = BinaryLogisticRegression(learning_rate=self.learning_rate, n_iterations=self.n_iterations)\n",
    "            model.fit(X, y_binary)\n",
    "            \n",
    "            self.model[cls] = model\n",
    "        print(f\"Trained {len(self.classes)} classifiers\")\n",
    "        return self\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        n_samples = len(X)\n",
    "        prob = np.zeros((n_samples, len(self.classes)))\n",
    "        \n",
    "        for i, cls in enumerate(self.classes):\n",
    "            prob[:, i] = self.model[cls].predict_prob(X)\n",
    "            \n",
    "        prob /= prob.sum(axis=1, keepdims=True)\n",
    "        return prob\n",
    "    \n",
    "    def predict(self, X):\n",
    "        prob = self.predict_prob(X)\n",
    "        return self.classes[np.argmax(prob, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e26796f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training 3 OvA binary classifiers...\n",
      "✓ Trained 3 classifiers\n",
      "\n",
      "OvA Results:\n",
      "  Train Accuracy: 0.9918\n",
      "  Test Accuracy: 0.9850\n"
     ]
    }
   ],
   "source": [
    "ova_model = OneVsAll(learning_rate=0.1, n_iterations=1000)\n",
    "ova_model.fit(X_train_normalized, y_train)\n",
    "\n",
    "y_train_pred_ova = ova_model.predict(X_train_normalized)\n",
    "y_test_pred_ova = ova_model.predict(X_test_normalized)\n",
    "\n",
    "train_acc_ova = np.mean(y_train_pred_ova == y_train)\n",
    "test_acc_ova = np.mean(y_test_pred_ova == y_test)\n",
    "\n",
    "print(f\"\\nOvA Results:\")\n",
    "print(f\"  Train Accuracy: {train_acc_ova:.4f}\")\n",
    "print(f\"  Test Accuracy: {test_acc_ova:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a1f6e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "class OneVsOne:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.model = {}\n",
    "        self.classes = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        \n",
    "        class_pairs = list(combinations(self.classes, 2))\n",
    "        print(f\"\\nTraining {len(class_pairs)} OvO binary classifiers...\")\n",
    "\n",
    "        for cls1, cls2 in class_pairs:\n",
    "            mask = (y == cls1) | (y == cls2)\n",
    "            X_pair = X[mask]\n",
    "            y_pair = y[mask]\n",
    "            \n",
    "            y_binary = (y_pair == cls1).astype(int)\n",
    "            \n",
    "            model = BinaryLogisticRegression(\n",
    "                learning_rate=self.learning_rate,\n",
    "                n_iterations=self.n_iterations\n",
    "            )\n",
    "            \n",
    "            model.fit(X_pair, y_binary)\n",
    "            self.model[(cls1, cls2)] = model\n",
    "            \n",
    "        print(f\"Trained {len(class_pairs)} classifiers\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        n_samples = len(X)\n",
    "        votes = np.zeros((n_samples, len(self.classes)))\n",
    "        \n",
    "        for (cls1, cls2), model in self.model.items():\n",
    "            prob = model.predict_prob(X)\n",
    "            \n",
    "            cls1_idx = self.classes.tolist().index(cls1)\n",
    "            cls2_idx = self.classes.tolist().index(cls2)\n",
    "            \n",
    "            # Class 1 gets votes where prob >= 0.5\n",
    "            votes[prob >= 0.5, cls1_idx] += 1\n",
    "            votes[prob < 0.5, cls2_idx] += 1\n",
    "        \n",
    "        return self.classes[np.argmax(votes, axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "54d190bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training 3 OvO binary classifiers...\n",
      "Trained 3 classifiers\n",
      "\n",
      "OvO Results:\n",
      "  Train Accuracy: 0.9918\n",
      "  Test Accuracy: 0.9850\n"
     ]
    }
   ],
   "source": [
    "ovo_model = OneVsOne(learning_rate=0.1, n_iterations=1000)\n",
    "ovo_model.fit(X_train_normalized, y_train)\n",
    "\n",
    "y_train_pred_ovo = ovo_model.predict(X_train_normalized)\n",
    "y_test_pred_ovo = ovo_model.predict(X_test_normalized)\n",
    "\n",
    "train_acc_ovo = np.mean(y_train_pred_ovo == y_train)\n",
    "test_acc_ovo = np.mean(y_test_pred_ovo == y_test)\n",
    "\n",
    "print(f\"\\nOvO Results:\")\n",
    "print(f\"  Train Accuracy: {train_acc_ovo:.4f}\")\n",
    "print(f\"  Test Accuracy: {test_acc_ovo:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d0ad382f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Softmax Results:\n",
      "  Train Accuracy: 0.9918\n",
      "  Test Accuracy: 0.9850\n"
     ]
    }
   ],
   "source": [
    "# Train Softmax\n",
    "softmax_model = SoftmaxLogisticRegression(learning_rate=0.1, n_iterations=1000)\n",
    "softmax_model.fit(X_train_normalized, y_train)\n",
    "\n",
    "y_train_pred_softmax = softmax_model.predict(X_train_normalized)\n",
    "y_test_pred_softmax = softmax_model.predict(X_test_normalized)\n",
    "\n",
    "train_acc_softmax = np.mean(y_train_pred_softmax == y_train)\n",
    "test_acc_softmax = np.mean(y_test_pred_softmax == y_test)\n",
    "\n",
    "print(f\"\\nSoftmax Results:\")\n",
    "print(f\"  Train Accuracy: {train_acc_softmax:.4f}\")\n",
    "print(f\"  Test Accuracy: {test_acc_softmax:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "52e16939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. TEST SET ACCURACY COMPARISON:\n",
      "   One-vs-All (OvA):     0.9850\n",
      "   One-vs-One (OvO):     0.9850\n",
      "   Softmax Regression:   0.9850\n",
      "\n",
      "2. TRAINING SET ACCURACY:\n",
      "   One-vs-All (OvA):     0.9918\n",
      "   One-vs-One (OvO):     0.9918\n",
      "   Softmax Regression:   0.9918\n",
      "\n",
      "3. CONVERGENCE BEHAVIOR:\n",
      "   OvA:       Trains 3 binary classifiers\n",
      "   OvO:       Trains 3 binary classifiers\n",
      "   Softmax:   Direct multi-class training\n",
      "\n",
      "4. COMPUTATIONAL COMPLEXITY:\n",
      "   OvA:       O(K) binary classifiers, where K=3\n",
      "   OvO:       O(K²) binary classifiers, where K(K-1)/2=3\n",
      "   Softmax:   O(1) multi-class classifier\n",
      "\n",
      "5. BEST PERFORMANCE:\n",
      "   Method: OvA\n",
      "   Test Accuracy: 0.9850\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n1. TEST SET ACCURACY COMPARISON:\")\n",
    "print(f\"   One-vs-All (OvA):     {test_acc_ova:.4f}\")\n",
    "print(f\"   One-vs-One (OvO):     {test_acc_ovo:.4f}\")\n",
    "print(f\"   Softmax Regression:   {test_acc_softmax:.4f}\")\n",
    "\n",
    "print(f\"\\n2. TRAINING SET ACCURACY:\")\n",
    "print(f\"   One-vs-All (OvA):     {train_acc_ova:.4f}\")\n",
    "print(f\"   One-vs-One (OvO):     {train_acc_ovo:.4f}\")\n",
    "print(f\"   Softmax Regression:   {train_acc_softmax:.4f}\")\n",
    "\n",
    "print(f\"\\n3. CONVERGENCE BEHAVIOR:\")\n",
    "print(f\"   OvA:       Trains {len(np.unique(y_train))} binary classifiers\")\n",
    "print(f\"   OvO:       Trains {len(list(combinations(np.unique(y_train), 2)))} binary classifiers\")\n",
    "print(f\"   Softmax:   Direct multi-class training\")\n",
    "\n",
    "print(f\"\\n4. COMPUTATIONAL COMPLEXITY:\")\n",
    "n_classes = len(np.unique(y_train))\n",
    "print(f\"   OvA:       O(K) binary classifiers, where K={n_classes}\")\n",
    "print(f\"   OvO:       O(K²) binary classifiers, where K(K-1)/2={(n_classes*(n_classes-1))//2}\")\n",
    "print(f\"   Softmax:   O(1) multi-class classifier\")\n",
    "\n",
    "best_method = ['OvA', 'OvO', 'Softmax'][np.argmax([test_acc_ova, test_acc_ovo, test_acc_softmax])]\n",
    "best_acc = max([test_acc_ova, test_acc_ovo, test_acc_softmax])\n",
    "print(f\"\\n5. BEST PERFORMANCE:\")\n",
    "print(f\"   Method: {best_method}\")\n",
    "print(f\"   Test Accuracy: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ec9860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Plots saved as 'task3_multiclass_classification.png'\n"
     ]
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Cost function convergence (Softmax)\n",
    "ax = axes[0, 0]\n",
    "ax.plot(softmax_model.cost_history, marker='o', color='purple', linewidth=2)\n",
    "ax.set_xlabel('Iterations (×100)')\n",
    "ax.set_ylabel('Cross-Entropy Loss')\n",
    "ax.set_title('Softmax Regression: Cost Function Convergence')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Accuracy Comparison\n",
    "ax = axes[0, 1]\n",
    "methods = ['OvA', 'OvO', 'Softmax']\n",
    "train_accs = [train_acc_ova, train_acc_ovo, train_acc_softmax]\n",
    "test_accs = [test_acc_ova, test_acc_ovo, test_acc_softmax]\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, train_accs, width, label='Train', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, test_accs, width, label='Test', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy Comparison: All Three Methods')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(methods)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim([0, 1.1])\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Plot 3: Confusion Matrix for best method\n",
    "ax = axes[1, 0]\n",
    "if best_method == 'OvA':\n",
    "    y_pred_best = y_test_pred_ova\n",
    "elif best_method == 'OvO':\n",
    "    y_pred_best = y_test_pred_ovo\n",
    "else:\n",
    "    y_pred_best = y_test_pred_softmax\n",
    "\n",
    "classes = np.unique(y_test)\n",
    "confusion = np.zeros((len(classes), len(classes)))\n",
    "for i, true_cls in enumerate(classes):\n",
    "    for j, pred_cls in enumerate(classes):\n",
    "        confusion[i, j] = np.sum((y_test == true_cls) & (y_pred_best == pred_cls))\n",
    "\n",
    "im = ax.imshow(confusion, cmap='Blues', aspect='auto')\n",
    "ax.set_xlabel('Predicted Class')\n",
    "ax.set_ylabel('True Class')\n",
    "ax.set_title(f'Confusion Matrix ({best_method})')\n",
    "ax.set_xticks(range(len(classes)))\n",
    "ax.set_yticks(range(len(classes)))\n",
    "ax.set_xticklabels(classes)\n",
    "ax.set_yticklabels(classes)\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    for j in range(len(classes)):\n",
    "        text = ax.text(j, i, int(confusion[i, j]),\n",
    "                      ha=\"center\", va=\"center\", color=\"black\", fontsize=12)\n",
    "\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Plot 4: Summary\n",
    "ax = axes[1, 1]\n",
    "ax.axis('off')\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "MULTI-CLASS CLASSIFICATION RESULTS\n",
    "\n",
    "Test Accuracy (Wine Dataset):\n",
    "  • One-vs-All (OvA):      {test_acc_ova:.4f}\n",
    "  • One-vs-One (OvO):      {test_acc_ovo:.4f}\n",
    "  • Softmax Regression:    {test_acc_softmax:.4f}\n",
    "  \n",
    "BEST: {best_method} ({best_acc:.4f})\n",
    "\n",
    "Data Summary:\n",
    "  • Samples: {len(y_train)} train, {len(y_test)} test\n",
    "  • Features: {X_train_normalized.shape[1]}\n",
    "  • Classes: {len(np.unique(y_train))}\n",
    "  • Outliers removed: {len(X) - len(X_train)}\n",
    "\n",
    "Training Details:\n",
    "  • Normalization: Min-Max [0, 1]\n",
    "  • Learning rate: 0.1\n",
    "  • Iterations: 1000\n",
    "  • Train/Test: 75/25 (stratified)\n",
    "\"\"\"\n",
    "\n",
    "ax.text(0.05, 0.95, summary_text, transform=ax.transAxes,\n",
    "        fontsize=10, verticalalignment='top', family='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('task3_multiclass_classification.png', dpi=150, bbox_inches='tight')\n",
    "print(\"Plots saved as 'task3_multiclass_classification.png'\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fe2d93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
